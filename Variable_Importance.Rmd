---
title: "Variable Importance"
author: "Jieqi Tu (jt3098)"
date: "3/1/2020"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(glmnet)
library(pROC)
library(xgboost)
library(randomForest)
```

## Import dataset
```{r import dataset}
# Import dataset
CL = readxl::read_excel("./data_new/ABC_Cord Blood_Metabolomics_CL data_15Jan2020.xlsx") 
BA = readxl::read_excel("./data_new/ABC_Cord Blood_Metabolomics_BA data_15Jan2020.xlsx") 
PM = readxl::read_excel("./data_new/ABC_Cord Blood_Metabolomics_PM data_15Jan2020.xlsx") 
OL =  readxl::read_excel("./data_new/ABC_Cord Blood_Metabolomics_OL data_15Jan2020.xlsx")
```


## Convert 0 to half of the minimum values
```{r convert 0 to half of the minimum values}
CL_data = CL[11:491]
BA_data = BA[11:266]
PM_data = PM[11:193]
OL_data = OL[11:81]
CL_data[CL_data == 0] = NA
BA_data[BA_data == 0] = NA
PM_data[PM_data == 0] = NA
OL_data[OL_data == 0] = NA
CL_min = sapply(CL_data[1:481], function(x) min(x, na.rm = T))
BA_min = sapply(BA_data[1:256], function(x) min(x, na.rm = T))
PM_min = sapply(PM_data[1:183], function(x) min(x, na.rm = T))
OL_min = sapply(OL_data[1:71], function(x) min(x, na.rm = T))

CL_data = CL[11:491]
BA_data = BA[11:266]
PM_data = PM[11:193]
OL_data = OL[11:81]
# Convert 0 to half of the minimum value
for(i in 1:481) {
  CL_data[i][CL_data[i]==0] = 0.5*CL_min[i]
}

for(i in 1:256) {
  BA_data[i][BA_data[i]==0] = 0.5*BA_min[i]
}

for(i in 1:183) {
  PM_data[i][PM_data[i]==0] = 0.5*PM_min[i]
}

for(i in 1:71) {
  OL_data[i][OL_data[i]==0] = 0.5*OL_min[i]
}

CL_info = CL[1:10]
CL_new = cbind.data.frame(CL_info, CL_data)

BA_info = BA[1:10]
BA_new = cbind.data.frame(BA_info, BA_data)

PM_info = PM[1:10]
PM_new = cbind.data.frame(PM_info, PM_data)

OL_info = OL[1:10]
OL_new = cbind.data.frame(OL_info, OL_data)
```

## Delete outliers
```{r outliers}
# Delete outliers identified in PCA
CL_new = CL_new[-c(39, 40, 55, 56, 121, 122, 199, 200),]
BA_new = BA_new[-c(5,6,41, 42, 191, 192, 121, 122, 125, 126, 195, 196, 199, 200),]
PM_new = PM_new[-c(31, 32),]
OL_new = OL_new[-c(159, 160, 123, 124, 29, 30, 85, 86),]
```

## Scaling and Transformation
```{r scaling and transformation}
# Base 10 log transformation
BA_log = BA_new
CL_log = CL_new
PM_log = PM_new
OL_log = OL_new
for(i in 1:256) {
  BA_log[i+10] = log10(BA_new[i+10])
}

for(i in 1:481) {
  CL_log[i+10] = log10(CL_new[i+10])
}

for(i in 1:183) {
  PM_log[i+10] = log10(PM_new[i+10])
}

for(i in 1:71) {
  OL_log[i+10] = log10(OL_new[i+10])
}
# Divide variables by the sd of control groups
BA_c = BA_log %>% 
  group_by(Strata) %>% 
  filter(PROT_ASD_2015 == 0)
CL_c = CL_log %>% 
  group_by(Strata) %>% 
  filter(PROT_ASD_2015 == 0)
PM_c = PM_log %>% 
  group_by(Strata) %>% 
  filter(PROT_ASD_2015 == 0)
OL_c = OL_log %>% 
  group_by(Strata) %>% 
  filter(PROT_ASD_2015 == 0)
sd_BA = sapply(BA_c[11:266], function(x) sd(x))
sd_CL = sapply(CL_c[11:491], function(x) sd(x))
sd_PM = sapply(PM_c[11:193], function(x) sd(x))
sd_OL = sapply(OL_c[11:81], function(x) sd(x))

# Divide the standard deviation
for(i in 1:256) {
  BA_log[i+10] = BA_log[i+10]/sd_BA[i]
}
for(i in 1:481) {
  CL_log[i+10] = CL_log[i+10]/sd_CL[i]
}
for(i in 1:183) {
  PM_log[i+10] = PM_log[i+10]/sd_PM[i]
}
for(i in 1:71) {
  OL_log[i+10] = OL_log[i+10]/sd_OL[i]
}

# Delete the matching variables
CL_analysis = CL_log[,-c(1, 2, 3, 4, 5, 6, 7, 8, 10)] 
BA_analysis = BA_log[,-c(1, 2, 3, 4, 5, 6, 7, 8, 10)] 
PM_analysis = PM_log[,-c(1, 2, 3, 4, 5, 6, 7, 8, 10)] 
OL_analysis = OL_log[,-c(1, 2, 3, 4, 5, 6, 7, 8, 10)] 

CL_analysis$PROT_ASD_2015 = ifelse(CL_analysis$PROT_ASD_2015 == "1", "Positive", "Negative")
BA_analysis$PROT_ASD_2015 = ifelse(BA_analysis$PROT_ASD_2015 == "1", "Positive", "Negative")
PM_analysis$PROT_ASD_2015 = ifelse(PM_analysis$PROT_ASD_2015 == "1", "Positive", "Negative")
OL_analysis$PROT_ASD_2015 = ifelse(OL_analysis$PROT_ASD_2015 == "1", "Positive", "Negative")
```
### Inititial setting
```{r control setting}
# Control setting
ctrl = trainControl(method = "cv",
                    summaryFunction = twoClassSummary,
                    classProbs = TRUE)
```

### Variable Importance for LASSO, Random Forest and XGBoost
```{r CL data}
set.seed(1029)

# Create a dataframe to store the importance measures
variable_lasso = data.frame(matrix(ncol = 2, nrow = 481))
colnames(variable_lasso) = c("variable_names", "numbers")
variable = colnames(CL_analysis)
variable_lasso$variable_names = variable[2:482]
variable_lasso$numbers = 0

variable_rf = data.frame(matrix(ncol = 2, nrow = 481))
colnames(variable_rf) = c("variable_names", "sum_mean_accuracy")
variable_rf$variable_names = variable[2:482]
variable_rf$sum_mean_accuracy = 0

variable_xgb_CL = data.frame(matrix(ncol = 2, nrow = 481))
colnames(variable_xgb_CL) = c("variable_names", "sum_gain")
variable_xgb_CL$variable_names = variable[2:482]
variable_xgb_CL$sum_gain = 0

CL_analysis$PROT_ASD_2015 = factor(CL_analysis$PROT_ASD_2015, levels = c("Negative", "Positive"))
# Randomly select 76 pairs out of 96 pairs
pairs = sample(1:96, 76)
rownum1 = pairs*2
rownum2 = rownum1 - 1
trainset_CL = CL_analysis[c(rownum1,rownum2),]
testset_CL = CL_analysis[-c(rownum1,rownum2),]

# Tune the parameter for LASSO regression
set.seed(1029)
lasso_tune = train(x = trainset_CL[2:482],
                   y = trainset_CL$PROT_ASD_2015,
                   method = "glmnet",
                   metric = "ROC",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-10,-1, length.out = 100))),
                   family = "binomial",
                   trControl = ctrl
)

plot(lasso_tune, xTrans = function(x) log(x))
lasso_tune$bestTune

# Tune XGboost
set.seed(1029)
xgboost.grid = expand.grid(eta = 0.1, 
                           colsample_bytree=c(0.6, 0.7, 0.8),
                           max_depth=c(1,2,3),
                           nrounds=100,
                           gamma=1,
                           min_child_weight=2,
                           subsample = 1)


model.xgboost = train(PROT_ASD_2015~., 
                     trainset_CL,
                     method = "xgbTree",
                     trControl = ctrl,
                     tuneGrid = xgboost.grid,
                     metric = "ROC",
                     verbose = F,
                     importance = "accuracy")

plot(model.xgboost)


# Tune Random Forest 
set.seed(1029)
rf.grid = expand.grid(mtry = 1:160,
                      splitrule = "gini",
                      min.node.size = 5)

rf_tune  = train(PROT_ASD_2015~., data = trainset_CL,
                 method = "ranger",
                 tuneGrid = rf.grid,
                 metric = "ROC",
                 trControl = ctrl,
                 importance = "permutation",
                 scale.permutation.importance = T)
ggplot(rf_tune, highlight = T)
rf_tune$bestTune

set.seed(1029)
for(i in 1:1000) {
newpairs = sample(1:76, 70, replace = TRUE)
row1 = newpairs * 2
row2 = row1 - 1
train_boot = trainset_CL[c(row1, row2),]

# Lasso
model.lasso = glmnet(x = as.matrix(train_boot[2:482]),
                     y = train_boot$PROT_ASD_2015,
                     family = "binomial",
                     alpha = 1,
                     lambda = 0.3678794)

coefficients = coef(model.lasso)
for (k in 2:482) {
  variable_lasso$numbers[k-1] = ifelse(coefficients[k,] != "0", variable_lasso$numbers[k-1]+1, variable_lasso$numbers[k-1])
}

# Random Forest
model.rf = randomForest(x = train_boot[2:482],
                        y = train_boot$PROT_ASD_2015,
                        data = train_boot,
                        ntree = 1000,
                        importance = T,
                        keep.forest = F,
                        mtry = 64)

rf_importance = importance(model.rf) %>% as.data.frame()
variable_rf$sum_mean_accuracy = variable_rf$sum_mean_accuracy + rf_importance$MeanDecreaseAccuracy

# XGBoost
model.xgboost = xgboost(data = as.matrix(train_boot[2:482]),
                        label = train_boot$PROT_ASD_2015,
                        nrounds = 100,
                        verbose = 0,
                        eta = 0.1,
                        gamma = 1, metric = "ROC",
                        max_depth = 2,
                        colsample_bytree = 0.6,
                        importance = "accuracy")
xgb_imp = xgb.importance(feature_names = variable[2:482], model = model.xgboost)

for (p in 1:nrow(xgb_imp)) {
 name = xgb_imp[[1]][p]
 variable_xgb_CL[variable_xgb_CL$variable_names == name,][[2]] = variable_xgb_CL[variable_xgb_CL$variable_names == name,][[2]] + xgb_imp[p,2]
}
}
# Sort the variable importance
sort_lasso_CL = variable_lasso[order(-variable_lasso$numbers),]
sort_rf_CL = variable_rf[order(-variable_rf$sum_mean_accuracy),]
variable_xgb_CL$sum_gain = as.numeric(variable_xgb_CL$sum_gain)
sort_xgb_CL = variable_xgb_CL[order(-variable_xgb_CL$sum_gain),]

# Join these three dataframes
lasso_rf_join = left_join(sort_lasso_CL, sort_rf_CL, by = "variable_names")
all_imp_CL = left_join(lasso_rf_join, sort_xgb_CL, by = "variable_names")
```

```{r variable importance for BA data}
set.seed(1029)

# Create a dataframe to store the importance measures
variable_lasso_BA = data.frame(matrix(ncol = 2, nrow = 256))
colnames(variable_lasso_BA) = c("variable_names", "numbers")
variable = colnames(BA_analysis)
variable_lasso_BA$variable_names = variable[2:257]
variable_lasso_BA$numbers = 0

variable_rf_BA = data.frame(matrix(ncol = 2, nrow = 256))
colnames(variable_rf_BA) = c("variable_names", "sum_mean_accuracy")
variable_rf_BA$variable_names = variable[2:257]
variable_rf_BA$sum_mean_accuracy = 0

variable_xgb_BA = data.frame(matrix(ncol = 2, nrow = 256))
colnames(variable_xgb_BA) = c("variable_names", "sum_gain")
variable_xgb_BA$variable_names = variable[2:257]
variable_xgb_BA$sum_gain = 0

BA_analysis$PROT_ASD_2015 = factor(BA_analysis$PROT_ASD_2015, levels = c("Negative", "Positive"))
# Randomly select 74 pairs out of 93 pairs
set.seed(1029)
pairs = sample(1:93, 74)
rownum1 = pairs*2
rownum2 = rownum1 - 1
trainset_BA = BA_analysis[c(rownum1,rownum2),]
testset_BA = BA_analysis[-c(rownum1,rownum2),]

# Tune the parameter for LASSO regression
set.seed(1029)
lasso_tune = train(x = trainset_BA[2:257],
                   y = trainset_BA$PROT_ASD_2015,
                   method = "glmnet",
                   metric = "ROC",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-8,-2, length.out = 100))),
                   family = "binomial",
                   trControl = ctrl
)

plot(lasso_tune, xTrans = function(x) log(x))
lasso_tune$bestTune

# Tune XGboost
set.seed(1029)
xgboost.grid = expand.grid(eta = 0.1, 
                           colsample_bytree=c(0.6, 0.7, 0.8),
                           max_depth=c(1, 2, 3, 4),
                           nrounds=100,
                           gamma=1,
                           min_child_weight=2,
                           subsample = 1)


model.xgboost = train(PROT_ASD_2015~., 
                     trainset_BA,
                     method = "xgbTree",
                     trControl = ctrl,
                     tuneGrid = xgboost.grid,
                     metric = "ROC",
                     verbose = F,
                     importance = "accuracy")

plot(model.xgboost)

# Tune Random Forest 
set.seed(1029)
rf.grid = expand.grid(mtry = 1:86,
                      splitrule = "gini",
                      min.node.size = 5)

rf_tune  = train(PROT_ASD_2015~., data = trainset_BA,
                 method = "ranger",
                 tuneGrid = rf.grid,
                 metric = "ROC",
                 trControl = ctrl,
                 importance = "permutation",
                 scale.permutation.importance = T)
ggplot(rf_tune, highlight = T)
rf_tune$bestTune

set.seed(1029)
for(i in 1:1000) {
newpairs = sample(1:74, 68, replace = TRUE)
row1 = newpairs * 2
row2 = row1 - 1
train_boot = trainset_BA[c(row1, row2),]

# Lasso
model.lasso = glmnet(x = as.matrix(train_boot[2:257]),
                     y = train_boot$PROT_ASD_2015,
                     family = "binomial",
                     alpha = 1,
                     lambda = 0.09995549)

coefficients = coef(model.lasso)
for (k in 2:257) {
  variable_lasso_BA$numbers[k-1] = ifelse(coefficients[k,] != "0", variable_lasso_BA$numbers[k-1]+1, variable_lasso_BA$numbers[k-1])
}

# Random Forest
model.rf = randomForest(x = train_boot[2:257],
                        y = train_boot$PROT_ASD_2015,
                        data = train_boot,
                        ntree = 1000,
                        importance = T,
                        keep.forest = F,
                        mtry = 82)

rf_importance = importance(model.rf) %>% as.data.frame()
variable_rf_BA$sum_mean_accuracy = variable_rf_BA$sum_mean_accuracy + rf_importance$MeanDecreaseAccuracy

# XGBoost
model.xgboost = xgboost(data = as.matrix(train_boot[2:257]),
                        label = train_boot$PROT_ASD_2015,
                        nrounds = 100,
                        verbose = 0,
                        eta = 0.1,
                        gamma = 1, metric = "ROC",
                        max_depth = 1,
                        colsample_bytree = 0.7,
                        importance = "accuracy")
xgb_imp = xgb.importance(feature_names = variable[2:257], model = model.xgboost)

for (p in 1:nrow(xgb_imp)) {
 name = xgb_imp[[1]][p]
 variable_xgb_BA[variable_xgb_BA$variable_names == name,][[2]] = variable_xgb_BA[variable_xgb_BA$variable_names == name,][[2]] + xgb_imp[p,2]
}
}
# Sort the variable importance
sort_lasso_BA = variable_lasso_BA[order(-variable_lasso_BA$numbers),]
sort_rf_BA = variable_rf_BA[order(-variable_rf_BA$sum_mean_accuracy),]
variable_xgb_BA$sum_gain = as.numeric(variable_xgb_BA$sum_gain)
sort_xgb_BA = variable_xgb_BA[order(-variable_xgb_BA$sum_gain),]

# Join all the variable importance
# Join these three dataframes
lasso_rf_join = left_join(sort_lasso_BA, sort_rf_BA, by = "variable_names")
all_imp_BA = left_join(lasso_rf_join, sort_xgb_BA, by = "variable_names")
```

```{r variable importance for PM data}
set.seed(1029)

# Create a dataframe to store the importance measures
variable_lasso_PM = data.frame(matrix(ncol = 2, nrow = 183))
colnames(variable_lasso_PM) = c("variable_names", "numbers")
variable = colnames(PM_analysis)
variable_lasso_PM$variable_names = variable[2:184]
variable_lasso_PM$numbers = 0

variable_rf_PM = data.frame(matrix(ncol = 2, nrow = 183))
colnames(variable_rf_PM) = c("variable_names", "sum_mean_accuracy")
variable_rf_PM$variable_names = variable[2:184]
variable_rf_PM$sum_mean_accuracy = 0

variable_xgb_PM = data.frame(matrix(ncol = 2, nrow = 183))
colnames(variable_xgb_PM) = c("variable_names", "sum_gain")
variable_xgb_PM$variable_names = variable[2:184]
variable_xgb_PM$sum_gain = 0

PM_analysis$PROT_ASD_2015 = factor(PM_analysis$PROT_ASD_2015, levels = c("Negative", "Positive"))
# Randomly select 77 pairs out of 97 pairs
set.seed(1029)
pairs = sample(1:97, 77)
rownum1 = pairs*2
rownum2 = rownum1 - 1
trainset_PM = PM_analysis[c(rownum1,rownum2),]
testset_PM = PM_analysis[-c(rownum1,rownum2),]

# Tune the parameter for LASSO regression
set.seed(1029)
lasso_tune = train(x = trainset_PM[2:184],
                   y = trainset_PM$PROT_ASD_2015,
                   method = "glmnet",
                   metric = "ROC",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-8,-2, length.out = 100))),
                   family = "binomial",
                   trControl = ctrl
)

plot(lasso_tune, xTrans = function(x) log(x))
lasso_tune$bestTune

# Tune XGboost
set.seed(1029)
xgboost.grid = expand.grid(eta = 0.1, 
                           colsample_bytree=c(0.6, 0.7, 0.8),
                           max_depth=c(1, 2, 3, 4),
                           nrounds=100,
                           gamma=1,
                           min_child_weight=2,
                           subsample = 1)


model.xgboost = train(PROT_ASD_2015~., 
                     trainset_BA,
                     method = "xgbTree",
                     trControl = ctrl,
                     tuneGrid = xgboost.grid,
                     metric = "ROC",
                     verbose = F,
                     importance = "accuracy")

plot(model.xgboost)

# Tune Random Forest 
set.seed(1029)
rf.grid = expand.grid(mtry = 1:61,
                      splitrule = "gini",
                      min.node.size = 5)

rf_tune  = train(PROT_ASD_2015~., data = trainset_PM,
                 method = "ranger",
                 tuneGrid = rf.grid,
                 metric = "ROC",
                 trControl = ctrl,
                 importance = "permutation",
                 scale.permutation.importance = T)
ggplot(rf_tune, highlight = T)
rf_tune$bestTune

set.seed(1029)
for(i in 1:1000) {
newpairs = sample(1:77, 70, replace = TRUE)
row1 = newpairs * 2
row2 = row1 - 1
train_boot = trainset_PM[c(row1, row2),]

# Lasso
model.lasso = glmnet(x = as.matrix(train_boot[2:184]),
                     y = train_boot$PROT_ASD_2015,
                     family = "binomial",
                     alpha = 1,
                     lambda = 0.02)

coefficients = coef(model.lasso)
for (k in 2:184) {
  variable_lasso_PM$numbers[k-1] = ifelse(coefficients[k,] != "0", variable_lasso_PM$numbers[k-1]+1, variable_lasso_PM$numbers[k-1])
}

# Random Forest
model.rf = randomForest(x = train_boot[2:184],
                        y = train_boot$PROT_ASD_2015,
                        data = train_boot,
                        ntree = 1000,
                        importance = T,
                        keep.forest = F,
                        mtry = 4)

rf_importance = importance(model.rf) %>% as.data.frame()
variable_rf_PM$sum_mean_accuracy = variable_rf_PM$sum_mean_accuracy + rf_importance$MeanDecreaseAccuracy

# XGBoost
model.xgboost = xgboost(data = as.matrix(train_boot[2:184]),
                        label = train_boot$PROT_ASD_2015,
                        nrounds = 100,
                        verbose = 0,
                        eta = 0.1,
                        gamma = 1, metric = "ROC",
                        max_depth = 1,
                        colsample_bytree = 0.7,
                        importance = "accuracy")
xgb_imp = xgb.importance(feature_names = variable[2:184], model = model.xgboost)

for (p in 1:nrow(xgb_imp)) {
 name = xgb_imp[[1]][p]
 variable_xgb_PM[variable_xgb_PM$variable_names == name,][[2]] = variable_xgb_PM[variable_xgb_PM$variable_names == name,][[2]] + xgb_imp[p,2]
}
}
# Sort the variable importance
sort_lasso_PM = variable_lasso_PM[order(-variable_lasso_PM$numbers),]
sort_rf_PM = variable_rf_PM[order(-variable_rf_PM$sum_mean_accuracy),]
variable_xgb_PM$sum_gain = as.numeric(variable_xgb_PM$sum_gain)
sort_xgb_PM = variable_xgb_PM[order(-variable_xgb_PM$sum_gain),]

# Join all the variable importance
# Join these three dataframes
lasso_rf_join = left_join(sort_lasso_PM, sort_rf_PM, by = "variable_names")
all_imp_PM = left_join(lasso_rf_join, sort_xgb_PM, by = "variable_names")
```

```{r variable importance for OL data}
set.seed(1029)

# Create a dataframe to store the importance measures
variable_lasso_OL = data.frame(matrix(ncol = 2, nrow = 71))
colnames(variable_lasso_OL) = c("variable_names", "numbers")
variable = colnames(OL_analysis)
variable_lasso_OL$variable_names = variable[2:72]
variable_lasso_OL$numbers = 0

variable_rf_OL = data.frame(matrix(ncol = 2, nrow = 71))
colnames(variable_rf_OL) = c("variable_names", "sum_mean_accuracy")
variable_rf_OL$variable_names = variable[2:72]
variable_rf_OL$sum_mean_accuracy = 0

variable_xgb_OL = data.frame(matrix(ncol = 2, nrow = 71))
colnames(variable_xgb_OL) = c("variable_names", "sum_gain")
variable_xgb_OL$variable_names = variable[2:72]
variable_xgb_OL$sum_gain = 0

OL_analysis$PROT_ASD_2015 = factor(OL_analysis$PROT_ASD_2015, levels = c("Negative", "Positive"))
# Randomly select 76 pairs out of 96 pairs
set.seed(1029)
pairs = sample(1:96, 76)
rownum1 = pairs*2
rownum2 = rownum1 - 1
trainset_OL = PM_analysis[c(rownum1,rownum2),]
testset_OL = PM_analysis[-c(rownum1,rownum2),]

# Tune the parameter for LASSO regression
set.seed(1029)
lasso_tune = train(x = trainset_OL[2:72],
                   y = trainset_OL$PROT_ASD_2015,
                   method = "glmnet",
                   metric = "ROC",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(-10,-2, length.out = 100))),
                   family = "binomial",
                   trControl = ctrl
)

plot(lasso_tune, xTrans = function(x) log(x))
lasso_tune$bestTune

# Tune XGboost
set.seed(1029)
xgboost.grid = expand.grid(eta = 0.1, 
                           colsample_bytree=c(0.6, 0.7, 0.8),
                           max_depth=c(1, 2, 3, 4, 5, 6),
                           nrounds=100,
                           gamma=1,
                           min_child_weight=2,
                           subsample = 1)


model.xgboost = train(PROT_ASD_2015~., 
                     trainset_OL,
                     method = "xgbTree",
                     trControl = ctrl,
                     tuneGrid = xgboost.grid,
                     metric = "ROC",
                     verbose = F,
                     importance = "accuracy")

plot(model.xgboost)

# Tune Random Forest 
set.seed(1029)
rf.grid = expand.grid(mtry = 1:24,
                      splitrule = "gini",
                      min.node.size = 1:5)

rf_tune  = train(PROT_ASD_2015~., data = trainset_OL,
                 method = "ranger",
                 tuneGrid = rf.grid,
                 metric = "ROC",
                 trControl = ctrl,
                 importance = "permutation",
                 scale.permutation.importance = T)
ggplot(rf_tune, highlight = T)
rf_tune$bestTune

set.seed(1029)
for(i in 1:1000) {
newpairs = sample(1:77, 70, replace = TRUE)
row1 = newpairs * 2
row2 = row1 - 1
train_boot = trainset_PM[c(row1, row2),]

# Lasso
model.lasso = glmnet(x = as.matrix(train_boot[2:72]),
                     y = train_boot$PROT_ASD_2015,
                     family = "binomial",
                     alpha = 1,
                     lambda = 0.03)

coefficients = coef(model.lasso)
for (k in 2:72) {
  variable_lasso_OL$numbers[k-1] = ifelse(coefficients[k,] != "0", variable_lasso_OL$numbers[k-1]+1, variable_lasso_OL$numbers[k-1])
}

# Random Forest
model.rf = randomForest(x = train_boot[2:72],
                        y = train_boot$PROT_ASD_2015,
                        data = train_boot,
                        ntree = 1000,
                        importance = T,
                        keep.forest = F,
                        mtry = 13)

rf_importance = importance(model.rf) %>% as.data.frame()
variable_rf_OL$sum_mean_accuracy = variable_rf_OL$sum_mean_accuracy + rf_importance$MeanDecreaseAccuracy

# XGBoost
model.xgboost = xgboost(data = as.matrix(train_boot[2:72]),
                        label = train_boot$PROT_ASD_2015,
                        nrounds = 100,
                        verbose = 0,
                        eta = 0.1,
                        gamma = 1, metric = "ROC",
                        max_depth = 1,
                        colsample_bytree = 0.7,
                        importance = "accuracy")
xgb_imp = xgb.importance(feature_names = variable[2:72], model = model.xgboost)

for (p in 1:nrow(xgb_imp)) {
 name = xgb_imp[[1]][p]
 variable_xgb_OL[variable_xgb_OL$variable_names == name,][[2]] = variable_xgb_OL[variable_xgb_OL$variable_names == name,][[2]] + xgb_imp[p,2]
}
}
# Sort the variable importance
sort_lasso_PM = variable_lasso_PM[order(-variable_lasso_PM$numbers),]
sort_rf_PM = variable_rf_PM[order(-variable_rf_PM$sum_mean_accuracy),]
variable_xgb_PM$sum_gain = as.numeric(variable_xgb_PM$sum_gain)
sort_xgb_PM = variable_xgb_PM[order(-variable_xgb_PM$sum_gain),]

# Join all the variable importance
# Join these three dataframes
lasso_rf_join = left_join(sort_lasso_PM, sort_rf_PM, by = "variable_names")
all_imp_PM = left_join(lasso_rf_join, sort_xgb_PM, by = "variable_names")
```

